{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfa.__version__)\n",
    "print(tfp.__version__)\n",
    "\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from os import walk\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import arff\n",
    "\n",
    "from pyts.approximation import SymbolicAggregateApproximation\n",
    "\n",
    "from modules import helper\n",
    "from modules import transformer\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random variables so that one run on the same computer always results in the same models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.RandomState(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "context.set_global_seed(seed_value)\n",
    "ops.get_default_graph().seed = seed_value\n",
    "\n",
    "#pip install tensorflow-determinism needed\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "np.random.seed(seed_value)\n",
    "#tf.experimental.numpy.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing the test and train data\n",
    "\n",
    "Change parameters for different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: http://www.timeseriesclassification.com/description.php?Dataset=SyntheticControl\n",
    "data_path_train = './Datasets/SyntheticControl/SyntheticControl_TRAIN.arff'\n",
    "data_path_test = './Datasets/SyntheticControl/SyntheticControl_TEST.arff'\n",
    "num_of_classes = 6\n",
    "seqSize = 60\n",
    "##source: http://www.timeseriesclassification.com/description.php?Dataset=ECG5000\n",
    "#data_path_train = './Datasets/ecg5000/ECG5000_TRAIN.arff'\n",
    "#data_path_test = './Datasets/ecg5000/ECG5000_TEST.arff'\n",
    "#num_of_classes = 5\n",
    "#seqSize = 140\n",
    "##source: http://www.timeseriesclassification.com/description.php?Dataset=Plane\n",
    "#data_path_train = './Datasets/Plane/Plane_TRAIN.arff'\n",
    "#data_path_test = './Datasets/Plane/Plane_TEST.arff'\n",
    "#num_of_classes = 7\n",
    "#seqSize = 144\n",
    "##source: http://www.timeseriesclassification.com/description.php?Dataset=PowerCons\n",
    "#data_path_train = './Datasets/PowerCons/PowerCons_TRAIN.arff'\n",
    "#data_path_test = './Datasets/PowerCons/PowerCons_TEST.arff'\n",
    "#num_of_classes = 2\n",
    "#seqSize = 144\n",
    "\n",
    "\n",
    "#Use saved model weights in the save folder\n",
    "useSaves = True\n",
    "\n",
    "#number of symbolics for SAX\n",
    "n_bins = 5\n",
    "\n",
    "#number of folds\n",
    "nrFolds = 5\n",
    "\n",
    "\n",
    "#Load and formate data\n",
    "data_train, meta_train = arff.loadarff(data_path_train)\n",
    "data_test, meta_test = arff.loadarff(data_path_test)\n",
    "\n",
    "data_train = np.array(data_train.tolist())\n",
    "data_test = np.array(data_test.tolist())\n",
    "\n",
    "y_trainy = data_train[:,-1].astype(int)\n",
    "y_train = []\n",
    "X_train = data_train[:,:-1]\n",
    "y_testy_full = data_test[:,-1].astype(int)\n",
    "y_testy = y_testy_full\n",
    "y_test = []\n",
    "X_test = data_test[:,:-1]\n",
    "\n",
    "X_train, y_trainy = shuffle(X_train, y_trainy, random_state = seed_value)\n",
    "\n",
    "for y in y_trainy:\n",
    "    y_train_puffer = np.zeros(num_of_classes)\n",
    "    y_train_puffer[y-1] = 1\n",
    "    y_train.append(y_train_puffer)\n",
    "\n",
    "for y in y_testy:\n",
    "    y_puffer = np.zeros(num_of_classes)\n",
    "    y_puffer[y-1] = 1\n",
    "    y_test.append(y_puffer)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.astype(float)\n",
    "y_test_full = np.array(y_test)\n",
    "y_test_full = y_test_full.astype(float)\n",
    "y_test = y_test_full  \n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "X_train = X_train.astype(float)\n",
    "\n",
    "# Initialize k-folds\n",
    "kf = StratifiedKFold(nrFolds, shuffle=True, random_state=seed_value) # Use for StratifiedKFold classification\n",
    "fold = 0\n",
    "\n",
    "# Earlystopping callback\n",
    "earlystop = EarlyStopping(monitor= 'val_loss', min_delta=0 , patience=70, verbose=0, mode='auto')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots to exam the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "z = 1\n",
    "\n",
    "for z in range(8,20):\n",
    "\n",
    "    x = range(seqSize)\n",
    "    y = X_train[z]\n",
    "    \n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    print(y_train[z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for attention combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doCombiStep(step, field, axis):\n",
    "    if(step == 'max'):\n",
    "        return np.max(field, axis=axis)\n",
    "    elif (step == 'sum'):\n",
    "        return np.sum(field, axis=axis)\n",
    "    \n",
    "\n",
    "# creating all cohrence attention combinations\n",
    "def makeAttention(outSax, x_train1, y_train1):\n",
    "\n",
    "    #predicted lables\n",
    "    attentionQ = outSax[9]\n",
    "    print('2222222222222')\n",
    "    print(len(attentionQ[2]))\n",
    "    print(len(attentionQ[2][0]))\n",
    "    print(len(attentionQ[2][0][0]))\n",
    "    \n",
    "    data_att = np.flip(np.array(attentionQ[2][0][0][5]), axis=0)\n",
    "    \n",
    "    if(order == 'lh'):\n",
    "        axis1 = 0\n",
    "        axis2 = 2\n",
    "    elif(order == 'hl'):\n",
    "        axis1 = 2\n",
    "        axis2 = 0\n",
    "    \n",
    "    attentionQ[1] = doCombiStep(step1, attentionQ[2], axis1)\n",
    "    attentionQ[1] = doCombiStep(step2, attentionQ[1], axis2) \n",
    "    \n",
    "    #compared to original predictions\n",
    "    #predictions = outSax[4].predict(outSax[6])\n",
    "    #predictions = np.argmax(predictions,axis=1) +1 \n",
    "    \n",
    "\n",
    "    #true lables\n",
    "    predictions = np.argmax(y_train1,axis=1) +1  \n",
    "\n",
    "\n",
    "    #nestest dict for saves\n",
    "    def nested_dict(n, type):\n",
    "        if n == 1:\n",
    "            return defaultdict(type)\n",
    "        else:\n",
    "            return defaultdict(lambda: nested_dict(n-1, type))\n",
    "\n",
    "    #position counter\n",
    "    rM = nested_dict(3, list)\n",
    "    #attention sum at each point\n",
    "    rMS = nested_dict(3, list)\n",
    "    #relative average at each point + more side combinations\n",
    "    rMA = nested_dict(3, list)\n",
    "\n",
    "\n",
    "    #put together all train attention to from symbol x to symbol y representation\n",
    "    z = 0\n",
    "    for index in range(len(attentionQ[1])):\n",
    "\n",
    "        data_word = np.array(x_train1).squeeze()[index]\n",
    "        X_ori = data_word\n",
    "        data_att = attentionQ[1][index]\n",
    "\n",
    "        for i in range(len(data_att)):\n",
    "            for j in range(len(data_att[i])):\n",
    "\n",
    "                if(len(rM[predictions[index]][X_ori[i]][X_ori[j]]) is 0):\n",
    "                    rM[predictions[index]][X_ori[i]][X_ori[j]] = np.zeros((len(data_att), len(data_att[i])))\n",
    "                    rMS[predictions[index]][X_ori[i]][X_ori[j]] = np.zeros((len(data_att), len(data_att[i])))\n",
    "                    rMA[predictions[index]][X_ori[i]][X_ori[j]] = np.zeros((len(data_att), len(data_att[i])))\n",
    "\n",
    "                    if len(rMA[predictions[index]]['x'][X_ori[j]]) is 0:\n",
    "                        rMA[predictions[index]]['x'][X_ori[j]] = np.zeros((len(data_att), len(data_att[i])))\n",
    "\n",
    "                    if len(rMA[predictions[index]]['xAvg'][X_ori[j]]) is 0:\n",
    "                        rMA[predictions[index]]['xAvg'][X_ori[j]] = np.zeros((len(data_att), len(data_att[i])))\n",
    "                if data_att[i][j] != 0:\n",
    "                    rM[predictions[index]][X_ori[i]][X_ori[j]][i][j] += 1           \n",
    "\n",
    "                #sum FCAM\n",
    "                rMS[predictions[index]][X_ori[i]][X_ori[j]][i][j] += data_att[i][j]\n",
    "                #CRCAM Sum\n",
    "                rMA[predictions[index]]['x'][X_ori[j]][i][j] += data_att[i][j]\n",
    "\n",
    "    valuesA = [-1, -0.5, 0, 0.5, 1]\n",
    "    for lable in rMA.keys():\n",
    "        for toL in valuesA:\n",
    "\n",
    "            for fromL in valuesA:\n",
    "                for j in range(len(data_att[i])):\n",
    "                    for i in range(len(data_att)): \n",
    "                        \n",
    "                        #FCAM r. average\n",
    "                        if rM[lable][fromL][toL][i][j] > 0:\n",
    "                            rMA[lable][fromL][toL][i][j] = rMS[lable][fromL][toL][i][j] / float(rM[lable][fromL][toL][i][j])\n",
    "                        else:\n",
    "                            rMA[lable][fromL][toL][i][j] = rMS[lable][fromL][toL][i][j]\n",
    "\n",
    "                        #CRCAM r. average\n",
    "                        if rM[lable][fromL][toL][i][j] > 0:\n",
    "                            rMA[lable]['xAvg'][toL][i][j] += rMS[lable][fromL][toL][i][j] / float(rM[lable][fromL][toL][i][j])\n",
    "                        else:\n",
    "                            rMA[lable]['xAvg'][toL][i][j] += rMS[lable][fromL][toL][i][j]\n",
    "\n",
    "            #GTM max of sum                \n",
    "            rMA[lable]['max'][toL] = np.max(rMA[lable]['x'][toL], axis=0) \n",
    "            #GTM average of sum         \n",
    "            rMA[lable]['average'][toL] = np.mean(rMA[lable]['x'][toL], axis=0) \n",
    "            #GTM median of sum         \n",
    "            rMA[lable]['median'][toL] = np.median(rMA[lable]['x'][toL], axis=0) \n",
    "            #GTM max of r.average          \n",
    "            rMA[lable]['max+'][toL] = np.max(rMA[lable]['xAvg'][toL], axis=0)  \n",
    "            #GTM average of r.average          \n",
    "            rMA[lable]['average+'][toL] = np.mean(rMA[lable]['xAvg'][toL], axis=0)\n",
    "            #GTM median of r.average         \n",
    "            rMA[lable]['median+'][toL] = np.median(rMA[lable]['xAvg'][toL], axis=0) \n",
    "    print('done')\n",
    "\n",
    "    return rMA, rMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate the full coherence matrices \n",
    "def classFullAtt(rMG, x_test, y_testy, inputKeys):\n",
    "    results = []\n",
    "    predictResults = []\n",
    "    allLableScores = []\n",
    "\n",
    "\n",
    "    maxFound = False\n",
    "\n",
    "    #sum maximum score\n",
    "    maxScores = dict()\n",
    "    for lable in rMG.keys():\n",
    "        maxis = []\n",
    "        for fromV in inputKeys:\n",
    "                maxis.append(np.max(list(rMG[lable][float(fromV)].values()), axis=0))\n",
    "        maxScores[lable]  = np.sum(np.max(maxis, axis=0))\n",
    "\n",
    "    print('done summing')\n",
    "\n",
    "    #sum normal score\n",
    "    for ti in range(len(x_test)):\n",
    "        trial = x_test[ti]\n",
    "        lableScores = dict()\n",
    "        \n",
    "        for lable in rMG.keys():\n",
    "            lableScores[lable] = 0\n",
    "\n",
    "        for fromVi in range(len(trial)):\n",
    "            fromV = trial[fromVi]\n",
    "            for toVi in range(len(trial)):\n",
    "                toV = trial[toVi]\n",
    "\n",
    "                for lable in rMG.keys():\n",
    "                    lableScores[lable] += rMG[lable][float(fromV)][float(toV)][fromVi][toVi]\n",
    "\n",
    "        maxFound = True\n",
    "\n",
    "        #get final score\n",
    "        for lable in rMG.keys():\n",
    "            lableScores[lable] = lableScores[lable]/maxScores[lable]\n",
    "\n",
    "        allLableScores.append(lableScores)\n",
    "\n",
    "        #classification\n",
    "        biggestValue = 0\n",
    "        biggestLable = np.nan\n",
    "        for lable in lableScores.keys():\n",
    "            if lableScores[lable] > biggestValue:\n",
    "                biggestValue = lableScores[lable]\n",
    "                biggestLable = lable\n",
    "        predictResults.append(biggestLable)\n",
    "        results.append(biggestLable == y_testy[ti])\n",
    "\n",
    "    print(sum(results)/len(results))\n",
    "    return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate the column reduced coherence matrices \n",
    "def xAttentionMatch(rMA, x_test, y_testy, inputKeys, key):    \n",
    "    results = []\n",
    "    predictResults = []\n",
    "    \n",
    "    #sum max score\n",
    "    maxScores = dict()\n",
    "    for lable in rMA.keys():\n",
    "            maxScores[lable] = np.sum(np.max(list(rMA[lable][key].values()), axis=0))\n",
    "    print('done summing')\n",
    "\n",
    "    #sum normal score\n",
    "    for ti in range(len(x_test)):\n",
    "        trial = x_test[ti]\n",
    "        lableScores = dict()\n",
    "        \n",
    "        for lable in rMA.keys():\n",
    "            lableScores[lable] = 0\n",
    "        \n",
    "        for fromVi in range(len(trial)):\n",
    "            for toVi in range(len(trial)):\n",
    "                toV = trial[toVi]\n",
    "\n",
    "                for lable in rMA.keys():\n",
    "                    lableScores[lable] += rMA[lable][key][float(toV)][fromVi][toVi]\n",
    "\n",
    "        #get final score                \n",
    "        for lable in rMA.keys():\n",
    "            lableScores[lable] = lableScores[lable]/maxScores[lable]\n",
    "            \n",
    "        #classification\n",
    "        biggestValue = 0\n",
    "        biggestLable = np.nan\n",
    "        for lable in lableScores.keys():\n",
    "            if lableScores[lable] > biggestValue:\n",
    "                biggestValue = lableScores[lable]\n",
    "                biggestLable = lable\n",
    "        predictResults.append(biggestLable)\n",
    "        results.append(biggestLable == y_testy[ti])\n",
    "\n",
    "    print(sum(results)/len(results))\n",
    "    return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate the minimal coherence matrix \n",
    "def calcFullAbstractAttention(reductionInt, x_test, rMA):\n",
    "    results = []\n",
    "    predictResults = []\n",
    "    \n",
    "    #all possible implemented reductions\n",
    "    reduceStrings = ['max','max+','average','average+','median','median+']\n",
    "    reduceString = reduceStrings[reductionInt]\n",
    "    \n",
    "    # calc normal and max scores\n",
    "    for ti in range(len(x_test)):\n",
    "        trial = x_test[ti]\n",
    "        lableScores = dict()\n",
    "        maxScores = dict()\n",
    "        for toVi in range(len(trial)):\n",
    "            toV = trial[toVi]\n",
    "\n",
    "            for lable in rMA.keys():\n",
    "                if lable in lableScores.keys():\n",
    "                    lableScores[lable] += rMA[lable][reduceString][float(toV)][toVi] \n",
    "                    maxScores[lable] += np.max(list(rMA[lable][reduceString].values()), axis=0)[toVi]\n",
    "                else:\n",
    "                    lableScores[lable] = rMA[lable][reduceString][float(toV)][toVi] \n",
    "                    maxScores[lable] =  np.max(list(rMA[lable][reduceString].values()), axis=0)[toVi]\n",
    "\n",
    "        #get final score\n",
    "        for lable in rMS.keys():\n",
    "            lableScores[lable] = lableScores[lable]/maxScores[lable]\n",
    "        \n",
    "        #classification\n",
    "        biggestValue = 0\n",
    "        biggestLable = np.nan\n",
    "        for lable in lableScores.keys():\n",
    "            if lableScores[lable] > biggestValue:\n",
    "                biggestValue = lableScores[lable]\n",
    "                biggestLable = lable\n",
    "        predictResults.append(biggestLable)\n",
    "        results.append(biggestLable == y_testy[ti])\n",
    "\n",
    "    #print(predictResults)\n",
    "    print(sum(results)/len(results))\n",
    "    return sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and process methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the transformer model with given information\n",
    "def createModel(splits, x_train, x_val, x_test, batchSize, num_of_classes, doMask= False, rate = 0, numOfAttentionLayers=2):    \n",
    "        print(np.array(x_train1).shape)\n",
    "        x_trains = np.dsplit(x_train, splits)\n",
    "        print(np.array(x_trains).shape)\n",
    "\n",
    "        x_trainsBatch = np.dsplit(x_train[:batchSize], splits)\n",
    "\n",
    "        x_tests = np.dsplit(x_test, splits)\n",
    "        x_vals = np.dsplit(x_val, splits)\n",
    "        maxLen = len(x_trains[0][0])\n",
    "        print(maxLen)\n",
    "\n",
    "        print(np.array(x_trains).shape)\n",
    "        flattenArray = []\n",
    "        inputShapes = []\n",
    "        encClasses = []\n",
    "        for i in range(len(x_trains)):\n",
    "            if doMask:\n",
    "\n",
    "                masky = createMask(x_trains[i], 6)\n",
    "                x_part = np.array(x_trains[i])\n",
    "                print(np.array(x_part).shape)\n",
    "            \n",
    "                seq_len1 = x_part.shape[1]\n",
    "\n",
    "                sens1 = x_part.shape[2]\n",
    "                input_shape1 = (seq_len1, sens1)\n",
    "                left_input1 = tf.keras.layers.Input(input_shape1, name='input_ids')\n",
    "                    \n",
    "                mask = tf.keras.layers.Input(shape=masky.shape[1:], name='attention_mask')\n",
    "                print('masky shape')\n",
    "                print(masky.shape)\n",
    "                print(mask)\n",
    "            else: \n",
    "                mask = Input(1)\n",
    "                x_part = np.array(x_trains[i])\n",
    "                print(np.array(x_part).shape)\n",
    "            \n",
    "                seq_len1 = x_part.shape[1]\n",
    "\n",
    "                sens1 = x_part.shape[2]\n",
    "                input_shape1 = (seq_len1, sens1)\n",
    "                left_input1 = Input(input_shape1)\n",
    "\n",
    "            inputShapes.append(left_input1)\n",
    "            if doMask:\n",
    "                inputShapes.append(mask)\n",
    "\n",
    "            encoded = left_input1\n",
    "            input_vocab_size = 0\n",
    "            \n",
    "            #create transformer encoder layer \n",
    "            encClass1 = transformer.Encoder(numOfAttentionLayers, 16, 6, 6, 5000, rate=rate, input_vocab_size = input_vocab_size + 2, maxLen = maxLen, seed_value=seed_value)\n",
    "                \n",
    "            encClasses.append(encClass1)\n",
    "\n",
    "            maskLayer = tf.keras.layers.Masking(mask_value=-2)\n",
    "            encInput = maskLayer(encoded)\n",
    "            enc1, attention, fullAttention = encClass1(encInput)\n",
    "            flatten1 = Flatten()(enc1)\n",
    "            flattenArray.append(flatten1)\n",
    "        \n",
    "\n",
    "        # Merge nets\n",
    "        if splits == 1:\n",
    "            merged = flattenArray[0]\n",
    "        else:\n",
    "            merged = concatenate(flattenArray)\n",
    "\n",
    "        output = Dense(num_of_classes, activation = \"sigmoid\")(merged)\n",
    "        \n",
    "        # Create combined model\n",
    "        wdcnnt_multi = Model(inputs=inputShapes,outputs=(output))\n",
    "        print(wdcnnt_multi.summary())\n",
    "        \n",
    "        print(wdcnnt_multi.count_params())\n",
    "        \n",
    "        tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=seed_value)\n",
    "\n",
    "        learning_rate = transformer.CustomSchedule(16)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.99, \n",
    "                                     epsilon=1e-9)\n",
    "        \n",
    "        wdcnnt_multi.compile(optimizer=optimizer,\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['accuracy'], run_eagerly=False)\n",
    "        \n",
    "        print('done')\n",
    "        \n",
    "        return wdcnnt_multi, inputShapes, x_trains, x_tests, x_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building saving name for model weights\n",
    "def getWeightName(learning = True):\n",
    "    baseName = \"./saves/weights-\" + str(data_path_train.split('/')[-1].split('.')[0]) + '-size' + str(seqSize) + '-threshold' + maxString + '-input' + abstractionString + '-fold' + str(fold)\n",
    "    if learning:\n",
    "        return baseName + '-learning.tf'\n",
    "    else:\n",
    "        return baseName + '.tf'\n",
    "\n",
    "# do training for the given model def\n",
    "def doAbstractedTraining(newTrain, newVal, newTest, abstraction = 0, earlyPredictorZ = None, takeAvg = True, rate=0, heatLayer = 0, numOfAttentionLayers = 1):\n",
    "    print('newTrain during abstract training:')\n",
    "    print(newTrain.shape)\n",
    "            \n",
    "    n_model2, inputs2, x_trains2, x_tests2, x_vals2 = createModel(1, newTrain, newVal, newTest , BATCH, num_of_classes, rate=rate, doMask=False, numOfAttentionLayers=numOfAttentionLayers)\n",
    "    weightsName = getWeightName(learning=True)\n",
    "    saveBest2 = transformer.SaveBest(weightsName)\n",
    "    \n",
    "    if (os.path.isfile(getWeightName(learning=False) + '.index') and useSaves):\n",
    "        print('found weights to load! Won\\'t train model!')\n",
    "        n_model2.load_weights(getWeightName(learning=False))\n",
    "    else:\n",
    "        print('No weights found! Start training model!')\n",
    "        n_model2.fit(x_trains2, y_train1, validation_data = (x_vals2, y_val) , epochs = 500, batch_size = BATCH, verbose=1, callbacks =[earlystop, saveBest2], shuffle = True)\n",
    "        n_model2.load_weights(getWeightName(learning=True))\n",
    "        n_model2.save_weights(getWeightName(learning=False), overwrite=True)\n",
    "        \n",
    "\n",
    "    earlyPredictor2 = tf.keras.Model(n_model2.inputs, n_model2.layers[2].output)\n",
    "\n",
    "    # Predictions on the validation set\n",
    "    predictions2 = n_model2.predict(x_vals2)\n",
    "    \n",
    "    print('############################')\n",
    "    predictions2 = np.argmax(predictions2,axis=1)\n",
    "\n",
    "    # Measure this fold's accuracy on validation set compared to actual labels\n",
    "    y_compare = np.argmax(y_val, axis=1)\n",
    "    val_score2 = metrics.accuracy_score(y_compare, predictions2) \n",
    "    print(f\"validation fold score with input {abstractionString}-{maxString}(accuracy): {val_score2}\")\n",
    "\n",
    "    # Predictions on the test set\n",
    "    limit = 300\n",
    "    test_predictions_loop2 = []\n",
    "    for bor in range(int(math.ceil(len(x_tests2[0])/limit))):\n",
    "        test_predictions_loop2.extend(n_model2.predict([x_tests2[0][bor*limit:(bor+1)*limit]]))\n",
    "\n",
    "    attentionQ0 = []\n",
    "    attentionQ1 = []\n",
    "    attentionQ2 = []\n",
    "\n",
    "    for bor in range(int(math.ceil(len(x_trains2[0])/limit))):\n",
    "        attOut = earlyPredictor2.predict([x_trains2[0][bor*limit:(bor+1)*limit]])\n",
    "        attentionQ0.extend(attOut[0]) \n",
    "        attentionQ1.extend(attOut[1])\n",
    "\n",
    "        if len(attentionQ2) == 0:\n",
    "            attentionQ2 = attOut[2]\n",
    "        else:\n",
    "            for k in range(len(attentionQ2)):\n",
    "                attentionQ2[k] = [x +y for x, y in zip(attentionQ2[k], attOut[2][k])]\n",
    "    \n",
    "\n",
    "    attentionQ2 = [attentionQ0, attentionQ1, attentionQ2]\n",
    "    \n",
    "    # Append actual labels of the test set to empty list\n",
    "    y_testyy = [y-1 for y in y_testy]\n",
    "    test_predictions_loop2 = np.argmax(test_predictions_loop2, axis=1)\n",
    "\n",
    "    # Measure this fold's accuracy on test set compared to actual labels\n",
    "    test_score2 = metrics.accuracy_score(y_testyy, test_predictions_loop2)\n",
    "    #print(test_predictions_loop2)\n",
    "\n",
    "    print(f\"test fold score with input {abstractionString}-(accuracy): {test_score2}\")\n",
    "    return val_score2, test_score2, predictions2, test_predictions_loop2, n_model2, inputs2, x_trains2, x_tests2, x_vals2, attentionQ2, earlyPredictor2, newTrain, newVal, newTest, y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(x_train1, x_val, X_test, y_train1, y_val, y_test, y_trainy, y_testy, binNr):    \n",
    "    \n",
    "    x_test = X_test.copy()\n",
    "    \n",
    "    processedDataName = \"./saves/\"+str(data_path_train.split('/')[-1].split('.')[0])+ '-size' + str(seqSize) + '-bin' + str(binNr)\n",
    "    fileExists = os.path.isfile(processedDataName +'.pkl')\n",
    "\n",
    "    if(fileExists and useSaves):\n",
    "        print('found file! Start loading file!')\n",
    "        res = helper.load_obj(processedDataName)\n",
    "\n",
    "\n",
    "        for index, v in np.ndenumerate(res):\n",
    "            print(index)\n",
    "            res = v\n",
    "        res.keys()\n",
    "\n",
    "        x_train1 = res['X_train']\n",
    "        #x_train1 = res['X_val']\n",
    "        x_test = res['X_test']\n",
    "        x_val = res['X_val']\n",
    "        X_train_ori = res['X_train_ori']\n",
    "        X_test_ori = res['X_test_ori']\n",
    "        y_trainy = res['y_trainy']\n",
    "        y_train1 = res['y_train']\n",
    "        y_test = res['y_test']\n",
    "        y_testy = res['y_testy']\n",
    "        y_val = res['y_val']\n",
    "        X_val_ori = res['X_val_ori']\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "        print(y_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print('SHAPES loaded')\n",
    "        \n",
    "    else:\n",
    "        print('SHAPES:')\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "        print(x_val.shape)\n",
    "        print(y_test.shape)\n",
    "        print(y_train.shape)\n",
    "        \n",
    "        trainShape = x_train1.shape\n",
    "        valShape = x_val.shape\n",
    "        testShape = x_test.shape\n",
    "        \n",
    "        scaler = StandardScaler()    \n",
    "        scaler = scaler.fit(x_train1.reshape((-1,1)))\n",
    "        X_train = scaler.transform(x_train1.reshape(-1, 1)).reshape(trainShape)\n",
    "        x_val = scaler.transform(x_val.reshape(-1, 1)).reshape(valShape)\n",
    "        x_test = scaler.transform(x_test.reshape(-1, 1)).reshape(testShape)\n",
    "\n",
    "        X_test_ori = x_test.copy()\n",
    "        X_val_ori = x_val.copy()\n",
    "        X_train_ori = x_train1.copy()\n",
    "\n",
    "\n",
    "        sax = SymbolicAggregateApproximation(n_bins=n_bins, strategy='uniform')\n",
    "        sax.fit(x_train1)\n",
    "\n",
    "        x_train1 = helper.symbolizeTrans(x_train1, sax)\n",
    "        x_val = helper.symbolizeTrans(x_val, sax)\n",
    "        x_test = helper.symbolizeTrans(x_test, sax)\n",
    "\n",
    "            \n",
    "\n",
    "        x_train1 = np.expand_dims(x_train1, axis=2)\n",
    "        x_val = np.expand_dims(x_val, axis=2)\n",
    "        x_test = np.expand_dims(x_test, axis=2)   \n",
    "        X_test_ori = np.expand_dims(X_test_ori, axis=2)   \n",
    "        X_train_ori = np.expand_dims(X_train_ori, axis=2) \n",
    "        X_val_ori = np.expand_dims(X_val_ori, axis=2) \n",
    "\n",
    "        print('saves shapes:')\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "\n",
    "        #save sax results to only calculate them once\n",
    "        resultsSave = {\n",
    "            'X_train':x_train1,\n",
    "            'X_train_ori':X_train_ori,\n",
    "            'X_test':x_test,\n",
    "            'X_test_ori':X_test_ori,\n",
    "            'X_val': x_val,\n",
    "            'X_val_ori':X_val_ori,\n",
    "            'y_trainy':y_trainy,\n",
    "            'y_train':y_train1,\n",
    "            'y_val': y_val,\n",
    "            'y_test':y_test,\n",
    "            'y_testy':y_testy\n",
    "        }\n",
    "        helper.save_obj(resultsSave, processedDataName)\n",
    "    return x_train1, x_val, x_test, y_train1, y_val, y_test, X_train_ori, X_val_ori, X_test_ori, y_trainy, y_testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start train and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize loop for every kth fold\n",
    "\n",
    "# Build empty lists for results\n",
    "\n",
    "#calculate globel coherence matrices\n",
    "doGlobalAbstraction = True\n",
    "#Attention layers count\n",
    "numOfAttentionLayers = 2\n",
    "#take Attention average\n",
    "takeAvg = True\n",
    "#drouput rate\n",
    "rate=0.3\n",
    "\n",
    "maxString = 'None'\n",
    "usedAbstraction = ['Ori', 'SAX']\n",
    "reduceString = ['max','max+','average','average+','median','median+']\n",
    "BATCH = 50\n",
    "fold = 0\n",
    "\n",
    "order = 'hl'\n",
    "step1 = 'max'\n",
    "step2 = 'sum'\n",
    "\n",
    "accResults = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "resultNames = ['Ori', 'SAX', 'full matries sum', 'full matries r. avg', 'colum reduced sum', 'column reduced r avg.', 'min matrix max',  'min matrix max+', 'min matrix avg', 'min matrix avg+', 'min matrix median', 'min matrix median+']\n",
    "\n",
    "for train, test in kf.split(X_train, y_trainy): \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    \n",
    "    #preprocess data\n",
    "    x_train1 = X_train[train]\n",
    "    x_val = X_train[test]\n",
    "    y_train1 = y_train[train]\n",
    "    y_trainy2 = y_trainy[train]\n",
    "    y_val = y_train[test]\n",
    "    \n",
    "    x_train1, x_val, x_test, y_train1, y_val, y_test, X_train_ori, X_val_ori, X_test_ori, y_trainy2, y_testy2 = preprocessData(x_train1, x_val, X_test, y_train1, y_val, y_test, y_trainy2, y_testy, fold)\n",
    "\n",
    "    \n",
    "    #ori data    \n",
    "    abstractionIndex = 0\n",
    "    resultIndex = 0\n",
    "    abstractionString = usedAbstraction[abstractionIndex]    \n",
    "    outOri = doAbstractedTraining(X_train_ori, X_val_ori, X_test_ori, abstractionIndex, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "    \n",
    "    accResults[resultIndex].append(outOri)\n",
    "    resultIndex+=1\n",
    "    \n",
    "    # sax data    \n",
    "    abstractionIndex += 1\n",
    "    abstractionString = usedAbstraction[abstractionIndex]  \n",
    "    outSax = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "    accResults[resultIndex].append(outSax)\n",
    "    resultIndex+=1\n",
    "    \n",
    "    #calc coherence matrices\n",
    "    if doGlobalAbstraction:\n",
    "        rMA, rMS = makeAttention(outSax, x_train1, y_train1)\n",
    "        \n",
    "\n",
    "        print('full attention coherences')\n",
    "        accResults[resultIndex].append(classFullAtt(rMS, x_test, y_testy, rMS[1].keys()))\n",
    "        resultIndex+=1 \n",
    "        accResults[resultIndex].append(classFullAtt(rMA, x_test, y_testy, rMS[1].keys()))\n",
    "        resultIndex+=1 \n",
    "        print('column reduced coherence matrices')\n",
    "        \n",
    "        accResults[resultIndex].append(xAttentionMatch(rMA, x_test, y_testy, rMS[1].keys(),'x'))\n",
    "        resultIndex+=1 \n",
    "        accResults[resultIndex].append(xAttentionMatch(rMA, x_test, y_testy, rMS[1].keys(),'xAvg'))\n",
    "        resultIndex+=1 \n",
    "        \n",
    "        print('minimal coherence matrix')\n",
    "        for r in range(len(reduceString)):\n",
    "            accResults[resultIndex].append(calcFullAbstractAttention(r, x_test, rMA))\n",
    "            resultIndex+=1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(len(accResults))[:2]:\n",
    "    resultName = resultNames[k]\n",
    "    results = accResults[k]\n",
    "    print('#########################################')\n",
    "    print(resultName + ' Scores:')\n",
    "    print('#########################################')\n",
    "    print(f\"Avg validation score (accuracy): {np.average([r[0] for r in results])}\")   \n",
    "    print([r[0] for r in results])\n",
    "    print(f\"Avg test score (accuracy): {np.average([r[1] for r in results])}\")\n",
    "    print([r[1] for r in results])\n",
    "    print('---------------------')\n",
    "print('#################')\n",
    "for k in range(len(accResults))[2:]:\n",
    "    resultName = resultNames[k]\n",
    "    results = accResults[k]\n",
    "    print('#########################################')\n",
    "    print(resultName + ' Scores:')\n",
    "    print('#########################################')\n",
    "    print(accResults[k])\n",
    "    print(np.average(accResults[k]))\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate attention value combinations for visualisations\n",
    "binNr = 1\n",
    "rMA, rMS = makeAttention(accResults[1][binNr], accResults[1][binNr][6], accResults[1][binNr][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot FCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valuesA = [-1, -0.5, 0, 0.5, 1]\n",
    "\n",
    "#which combination to display\n",
    "combination = 'ravg'  #['sum', 'ravg']\n",
    "\n",
    "for lable in range(1,7):\n",
    "    print('lable: '+ str(lable))\n",
    "    fig, ax = plt.subplots(figsize=(60,30), nrows=len(valuesA), ncols=len(valuesA))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for rowI, row in enumerate(ax):\n",
    "        for colI, col in enumerate(row):\n",
    "            fromV = valuesA[rowI]\n",
    "            toV = valuesA[colI]\n",
    "            \n",
    "            if combination == 'sum':\n",
    "                data_att = rMS[lable][fromV][toV]\n",
    "            elif combination == 'ravg':\n",
    "                data_att = rMA[lable][fromV][toV]\n",
    "            data_att = np.flip(np.array(data_att), axis=0)\n",
    "            data_wordF = [fromV] * len(rMA[lable][fromV][toV])\n",
    "            data_wordT = [toV] * len(rMA[lable][fromV][toV])\n",
    "            d = pd.DataFrame(data = data_att,index = data_wordF, columns=data_wordT)\n",
    "            sns.heatmap(d, vmin=0, vmax=0.22, ax=col, cmap=\"OrRd\")\n",
    "\n",
    "    \n",
    "    #plt.savefig('./Bilder/full/Class' + str(lable) +'fullmatrix.png', dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot CRCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valuesA = [-1, -0.5, 0, 0.5, 1]\n",
    "\n",
    "combination = 'xAvg'  #['x', 'xAvg']\n",
    "\n",
    "for lable in range(1,7):\n",
    "\n",
    "    print(lable)\n",
    "    fig, ax = plt.subplots(figsize=(60,30), nrows=len(valuesA), ncols=1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for rowI, row in enumerate(reversed(ax)):\n",
    "            toV = valuesA[rowI]\n",
    "            att_sum = rMA[lable][combination][toV]\n",
    "            att_sum = np.flip(np.array(att_sum), axis=0)\n",
    "            data_att = att_sum\n",
    "            data_wordF = ['x'] * len(rMA[lable][fromV][toV])\n",
    "            data_wordT = [toV] * len(rMA[lable][fromV][toV])\n",
    "            d = pd.DataFrame(data = data_att,index = data_wordF, columns=data_wordT)\n",
    "\n",
    "            sns.heatmap(d, vmin=0, vmax=0.68, ax=row, cmap=\"OrRd\")\n",
    "            plt.tick_params(labelsize=26)\n",
    "            label_y = row.get_yticklabels()\n",
    "            plt.setp(label_y, rotation=360, horizontalalignment='right')\n",
    "            label_x = row.get_xticklabels()\n",
    "            plt.setp(label_x, rotation=45, horizontalalignment='right')\n",
    "            \n",
    "            plt.xlabel(\"X axis label\")\n",
    "    #plt.savefig('./Bilder/Red/Class' + str(lable) +'colmMatrix.png', dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot GTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valuesA = [-1, -0.5, 0, 0.5, 1]\n",
    "reductionWith = reduceString[2] # ['max','max+','average','average+','median','median+']\n",
    "\n",
    "for lable in rMS.keys():\n",
    "    print('lable: '+ str(lable))\n",
    "\n",
    "\n",
    "    data_att = np.flip(np.array(list(rMA[lable][reductionWith].values())), axis=0)\n",
    "    data_wordF = [np.flip(np.array(valuesA))]\n",
    "    data_wordT = range(len(data_att[0])) \n",
    "    d = pd.DataFrame(data = data_att,index = data_wordF, columns=data_wordT)\n",
    "    f, ax = plt.subplots(figsize=(60,30))\n",
    "    sns.heatmap(d, vmin=0, vmax=0.88, ax=ax, cmap=\"OrRd\")\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.setp(label_y, rotation=360, horizontalalignment='right')\n",
    "    label_x = ax.get_xticklabels()\n",
    "    plt.setp(label_x, rotation=45, horizontalalignment='right')\n",
    "    plt.tick_params(labelsize=26)\n",
    "    #plt.savefig('./Bilder/min/Class' + str(lable) +'minMatrix.png', dpi = 300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
